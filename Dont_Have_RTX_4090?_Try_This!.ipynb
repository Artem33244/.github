{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMylHQfH+66YHlPFPlpwPyu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Artem33244/.github/blob/main/Dont_Have_RTX_4090%3F_Try_This!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifh-ED87Gexn"
      },
      "outputs": [],
      "source": [
        "#Пайп лайны взяты https://huggingface.co/docs/diffusers\n",
        "#Обязательно в среде выполнение поставьте себе GPU перед началом\n",
        "!pip install diffusers[\"torch\"] transformers\n",
        "!pip install accelerate\n",
        "!pip install git+https://github.com/huggingface/diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ТЕКСТ В ИЗОБРАЖЕНИЕ**"
      ],
      "metadata": {
        "id": "upDspsjPSwoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "# Вместо \"redstonehero/cetusmix_v4\" вставляется название модели. В основном найти модели можно на сайте https://civitai.com/ от туда уже ищите модель на сайте https://huggingface.co/,\n",
        "# Когда зашли на нужную нам модель на хаге в ссылке страницы вы увидите https://huggingface.co/ByteDance/SDXL-Lightning\n",
        "# из ссылки нам надо получить ByteDance/SDXL-Lightning и вставить сюда вместо redstonehero/cetusmix_v4\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"redstonehero/cetusmix_v4\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.safety_checker = None\n",
        "#То что вы хотите получить, то что в начале строки генерируется первым, можно использовать скобки (word) ((word)) (((word))) для изменения приоритета внутри запроса\n",
        "prompt = \"medival fortress, landscape, scenery, 8K, masterpiece, ultradetailed\"\n",
        "# Высота и ширина рекомендуется не делать слишком большое.\n",
        "h=800\n",
        "w=640\n",
        "# Количество шагов по созданию изображения. Чем их больше, тем дольше нейросеть будет над ней работать но тем более детализированее будет изображение, лучше не выходить за пределы 40 шагов, может дать обратный эффект.\n",
        "steps=25\n",
        "# Точность, чем выше тем больше точность генерации, рекомендуется оставить на значении 7,5\n",
        "guidance=7.5\n",
        "# То что вы не хотите получить на фотографии\n",
        "neg = \"easynegative, human, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worstquality, low quality, normal quality, jpegartifacts, signature, watermark, username, blurry, bad feet, cropped, poorly drawn hands, poorly drawn face, mutation, deformed, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, extra fingers, fewer digits, extra limbs, extra arms,extra legs, malformed limbs, fused fingers, too many fingers, long neck, cross-eyed,mutated hands, polar lowres, bad body, bad proportions, gross proportions, text, error, missing fingers, missing arms, missing legs, extra digit, extra arms, extra leg, extra foot,\"\n",
        "\n",
        "image = pipe(prompt, height=h, width=w, num_inference_steps=steps, guidance_scale=guidance, negative_prompt=neg).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "rnNxgw_PGh3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ИЗОБРАЖЕНИЕ В ИЗОБРАЖЕНИЕ**"
      ],
      "metadata": {
        "id": "XbM9UB0_SlPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import pipeline\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "\n",
        "image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg\" # Вставьте сюда ссылку на ваше изображение\n",
        ")\n",
        "\n",
        "def get_depth_map(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    detected_map = torch.from_numpy(image).float() / 255.0\n",
        "    depth_map = detected_map.permute(2, 0, 1)\n",
        "    return depth_map\n",
        "\n",
        "depth_estimator = pipeline(\"depth-estimation\")\n",
        "depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to(\"cuda\")"
      ],
      "metadata": {
        "id": "ld9sHpwPQmS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "import torch\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", torch_dtype=torch.float16, use_safetensors=True)\n",
        "pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True # На этой строчке вы можете подгрузить модель как в примере выше\n",
        ")\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_model_cpu_offload()"
      ],
      "metadata": {
        "id": "CzfKqysCR0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(\n",
        "    \"lego batman and robin\", image=image, control_image=depth_map, ## ПРОМПТ К ИЗОБРАЖЕНИЮ\n",
        ").images[0]\n",
        "make_image_grid([image, output], rows=1, cols=2)"
      ],
      "metadata": {
        "id": "1U0xtjaeSjXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}